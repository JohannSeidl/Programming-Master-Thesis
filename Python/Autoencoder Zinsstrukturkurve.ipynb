{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese File dient der Erstellung und Speicherung eines AE. Die Auswertung des AE kann in R mittels der geeigneten Funktionen erfolgen. \n",
    "\n",
    "Input, welcher ben√∂tigt wird: exportFamaBliss_final30.csv, exportFamaBlisstimeonly30.csv, exportFamaBlissyieldsonly30.csv, mattimes30.csv und rownum30.csv\n",
    "\n",
    "Output: export_yieldsx.xlsx, Encoder, Decoder, AE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activ = 'tanh'\n",
    "inputnodes = 30\n",
    "bottleneck = 3\n",
    "\n",
    "encoder_input_y = keras.Input(shape=(inputnodes), name=\"Spot-Rates\")  \n",
    "encoder_input_t = keras.Input(shape=(inputnodes), name=\"Laufzeit\")\n",
    "\n",
    "x = layers.concatenate([encoder_input_y, encoder_input_t], name=\"Zsmf.1\")\n",
    "#x = layers.Dense(62,activation=activ)(x)\n",
    "x = layers.Dense(54,activation=activ)(x)\n",
    "x = layers.Dense(42,activation=activ)(x)\n",
    "x = layers.Dense(30,activation=activ)(x)\n",
    "x = layers.Dense(18,activation=activ)(x)\n",
    "x = layers.Dense(12,activation=activ)(x)\n",
    "x = layers.Dense(6,activation=activ)(x)\n",
    "\n",
    "\n",
    "encoder_output = layers.Dense(bottleneck,activation=activ)(x)\n",
    "\n",
    "encoder = keras.Model(inputs=[encoder_input_y, encoder_input_t], outputs=encoder_output, name=\"Encoder\")\n",
    "#encoder.summary()\n",
    "\n",
    "\n",
    "bottleneck_input = keras.Input(shape=(bottleneck,), name=\"Engpass\") \n",
    "time_input = keras.Input(shape=(inputnodes), name=\"Laufzeit\")\n",
    "\n",
    "\n",
    "### Transforming the Data\n",
    "## Encoder Output\n",
    "#Coping the nodes of the encoder part next to each other. Extending the vector by one dim to a None x 3 x 1\n",
    "reforminput1 = tf.keras.backend.expand_dims(bottleneck_input,axis=-1)\n",
    "reforminput1 = tf.tile(reforminput1, multiples = [1,1,inputnodes])\n",
    "# Shape of test2 is now Nonex3x30\n",
    "\n",
    "## Time Input\n",
    "encoder_input_t_2 = tf.keras.backend.expand_dims(time_input,axis=-1)\n",
    "encoder_input_t_2 = tf.keras.backend.permute_dimensions(encoder_input_t_2, (0,2,1))\n",
    "# Shape of encoder_input_t_2 is now None x 1 x inputnodes\n",
    "\n",
    "## concatenate along axis = 1  \n",
    "\n",
    "inputdecoder = layers.concatenate([reforminput1, encoder_input_t_2], name=\"Zsmf.2\", axis=1)\n",
    "\n",
    "## Decoder\n",
    "x = tf.keras.backend.permute_dimensions(inputdecoder, (0,2,1))\n",
    "#x = layers.Dense(4,activation=activ)(x)\n",
    "#x = layers.Dense(4,activation=activ)(x)\n",
    "x = layers.Dense(3,activation=activ)(x)\n",
    "x = layers.Dense(2,activation=activ)(x)\n",
    "x = layers.Dense(1,activation=activ)(x)\n",
    "decoder_output = layers.Flatten(name=\"Spot-Rates\")(x)\n",
    "\n",
    "\n",
    "decoder = keras.Model(inputs=[bottleneck_input,time_input],outputs=decoder_output, name=\"Decoder\")\n",
    "\n",
    "encoded_values = encoder([encoder_input_y,encoder_input_t])\n",
    "decoded_yields = decoder([encoded_values,encoder_input_t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file name\n",
    "filename = 'exportFamaBliss_final30.csv'\n",
    " \n",
    "# initializing the titles and rows list\n",
    "fields = []\n",
    "x = []\n",
    "\n",
    "with open(filename, newline='') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "     \n",
    "    # extracting field names through first row\n",
    "    fields = next(csvreader)\n",
    " \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        x.append(row)\n",
    "\n",
    "y = []\n",
    "filename2 = 'exportFamaBlissyieldsonly30.csv'\n",
    "\n",
    "with open(filename2, newline='') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "     \n",
    "    # extracting field names through first row\n",
    "    fields = next(csvreader)\n",
    " \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        y.append(row)   \n",
    "        \n",
    "ti = []\n",
    "filename3 = 'exportFamaBlisstimeonly30.csv'\n",
    "\n",
    "with open(filename3, newline='') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "     \n",
    "    # extracting field names through first row\n",
    "    fields = next(csvreader)\n",
    " \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        ti.append(row)   \n",
    "        \n",
    "#print(wichtig[0])\n",
    "#len(wichtig)\n",
    "#print(x[0])\n",
    "\n",
    "for i in range(len(x)):\n",
    "    x[i] = np.array_split(x[i], 2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = keras.Model(inputs=[encoder_input_y, encoder_input_t],outputs=decoded_yields, name=\"Autoencoder\")\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse',metrics=['accuracy'])\n",
    "\n",
    "x1 = pad_sequences(x,maxlen=None,dtype='float64')\n",
    "y1 = pad_sequences(y,maxlen=None,dtype='float64')\n",
    "ti1 = pad_sequences(ti,maxlen=None,dtype='float64')\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#scaledy1 = scaler.fit_transform(y1)\n",
    "#scaledti1 = scaler.fit_transform(ti1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(ti1, y1, test_size=0.15, random_state=1)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "with open('x_train.pickle', 'wb') as output:\n",
    "    pickle.dump(x_train, output)\n",
    "    \n",
    "with open('x_test.pickle', 'wb') as output:\n",
    "    pickle.dump(x_test, output)\n",
    "    \n",
    "with open('x_validation.pickle', 'wb') as output:\n",
    "    pickle.dump(x_validation, output)\n",
    "    \n",
    "with open('y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "with open('y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "with open('y_validation.pickle', 'wb') as output:\n",
    "    pickle.dump(y_validation, output)\n",
    "    \n",
    "    \n",
    "history = model.fit(x=[y_train,x_train], y=y_train, epochs=3000, batch_size=32, validation_data=([y_validation,x_validation], y_validation))\n",
    "#history = model.fit(x=[y_train,x_train], y=y_train, epochs=1000, batch_size=64)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "with open('history.pickle', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'][350:])\n",
    "#plt.plot(history.history['val_loss'][350:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate([y_test,x_test],y_test, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict([y_test,x_test])\n",
    "#predictions = scaler.inverse_transform(predictions)\n",
    "print(\"predictions shape:\", predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array111 = encoder.predict([y1,ti1])\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(array111, columns = ['x1','x2','x3'])\n",
    "df.to_excel (r'export_dataframe1.xlsx', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiexport = []\n",
    "filename4 = 'mattimes30.csv'\n",
    "\n",
    "with open(filename4, newline='') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "     \n",
    "    # extracting field names through first row\n",
    "    fields = next(csvreader)\n",
    " \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        tiexport.append(row)   \n",
    "        \n",
    "        \n",
    "rownums = []\n",
    "filename5 = 'rownum30.csv'\n",
    "\n",
    "with open(filename5, newline='') as csvfile:\n",
    "    # creating a csv reader object\n",
    "    csvreader = csv.reader(csvfile)\n",
    "     \n",
    "    # extracting field names through first row\n",
    "    fields = next(csvreader)\n",
    " \n",
    "    # extracting each data row one by one\n",
    "    for row in csvreader:\n",
    "        rownums.append(row)   \n",
    "        \n",
    "rownums = pad_sequences(rownums,maxlen=None,dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayfinal = []\n",
    "tiexport1 = pad_sequences(tiexport,maxlen=None,dtype='float64')\n",
    "\n",
    "for i in range(len(array111)):\n",
    "    if i==0:\n",
    "        a = 0\n",
    "        b = int(rownums[0][0])\n",
    "        tiexport1_new = tiexport1[a:b]\n",
    "        input1 = []\n",
    "        for _ in range(len(tiexport1_new)):\n",
    "            input1.append(array111[i])\n",
    "        \n",
    "        input1 = pad_sequences(input1,maxlen=None,dtype='float64')\n",
    "        array222 = decoder.predict([input1,tiexport1_new])\n",
    "        \n",
    "        arrayfinal.append(array222)\n",
    "    else:\n",
    "        a = int(sum(rownums[0:i])[0])\n",
    "        b = int(sum(rownums[0:(i+1)])[0])\n",
    "\n",
    "        tiexport1_new = tiexport1[a:b]\n",
    "        input1 = []\n",
    "        for _ in range(len(tiexport1_new)):\n",
    "            input1.append(array111[i])\n",
    "        \n",
    "        input1 = pad_sequences(input1,maxlen=None,dtype='float64')\n",
    "        array222 = decoder.predict([input1,tiexport1_new])\n",
    "        \n",
    "        arrayfinal.append(array222)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arraytestt = []\n",
    "for i in range(len(arrayfinal)):\n",
    "    for j in range(len(arrayfinal[i])):\n",
    "        arraytestt.append(arrayfinal[i][j])\n",
    "\n",
    "arrayfin = pd.DataFrame(arraytestt)\n",
    "arrayfin.to_excel (r'export_yields0.xlsx', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"AutoenconderModel0.png\", show_shapes=True)\n",
    "keras.utils.plot_model(encoder, \"Encoder0.png\", show_shapes=True)\n",
    "keras.utils.plot_model(decoder, \"Decoder0.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('encoder0')\n",
    "decoder.save('decoder0')\n",
    "model.save('autoencoder0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = keras.models.load_model(\"autoencoder0\")\n",
    "\n",
    "# Let's check:\n",
    "np.testing.assert_allclose(\n",
    "    model.predict([y_test,x_test]), reconstructed_model.predict([y_test,x_test])\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
